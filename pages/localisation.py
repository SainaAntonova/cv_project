import streamlit as st
from PIL import Image
import torch
from torch import nn
from torchvision.models import ResNet18_Weights, resnet18
import cv2
import numpy as np
import time 

import torchvision.transforms as T

preprocessing_func = T.Compose(
    [
        T.Resize((227,227)),
        T.ToTensor()
    ]
)

def preprocess(img):
    return preprocessing_func(img)




class Classifier(nn.Module):
    def __init__(self):
        super().__init__()

        self.feature_extractor = resnet18(weights=ResNet18_Weights.DEFAULT)
        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-2])
        # —Ñ—Ä–∏–∑–∏–º —Å–ª–æ–∏, –æ–±—É—á–∞—Ç—å –∏—Ö –Ω–µ –±—É–¥–µ–º (—Ö–æ—Ç—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –º–æ–∂–Ω–æ)
        for param in self.feature_extractor.parameters():
            param.requires_grad = True

        self.clf = nn.Sequential(
            nn.Linear(512*8*8, 128),
            nn.Sigmoid(),
            nn.Linear(128, 3)
        )

        self.box = nn.Sequential(
            nn.Linear(512*8*8, 128),
            nn.Sigmoid(),
            nn.Linear(128, 4),
            nn.Sigmoid()
        )

    def forward(self, img):
        resnet_out = self.feature_extractor(img)
        resnet_out = resnet_out.view(resnet_out.size(0), -1)
        pred_classes = self.clf(resnet_out)
        pred_boxes = self.box(resnet_out)
        return pred_classes, pred_boxes
    
device = 'cuda' if torch.cuda.is_available() else 'cpu'

@st.cache_data
def load_model():
    
    model = Classifier()

    checkpoint_path = 'notebooks/model/loc_model/best.pth'
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint)
    model.to(device)
    model.eval()

    return model


model = load_model()

st.image("https://previews.123rf.com/images/foodandmore/foodandmore1705/foodandmore170500090/77253581-panoramic-wide-organic-food-background-concept-with-full-frame-pile-of-fresh-vegetables-and-fruits.jpg", use_column_width=True)
st.markdown("""
    <style>
        .title {
            font-family: 'Arial Black', sans-serif;
            font-size: 36px;
            color: #1E90FF;
            text-align: center;
            margin-bottom: 30px;
        }
        .sidebar .sidebar-content {
            background-image: linear-gradient(#2e7bcf,#2e7bcf);
            color: white;
        }
        .stButton>button {
            color: white;
            background: linear-gradient(to right, #1fa2ff, #12d8fa, #a6ffcb);
        }
        .uploaded-image {
            border: 5px solid #1E90FF;
            border-radius: 10px;
            margin-bottom: 15px;
        }
    </style>
    """, unsafe_allow_html=True)

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
st.markdown('<h1 class="title">üì∑ –õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ ResNet-18</h1>', unsafe_allow_html=True)
st.write("""
         –≠—Ç–æ—Ç –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ ResNet-18.
         –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∏ –ª–æ–∫–∞–ª–∏–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏, –æ—Ç–æ–±—Ä–∞–∂–∞—è –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã–µ —Ä–∞–º–∫–∏ –≤–æ–∫—Ä—É–≥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤.
    """)

with st.expander("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏"):
    st.write('## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏')
    st.write('- –ß–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤: 3 - –æ–≥—É—Ä–µ—Ü, –±–∞–∫–ª–∞–∂–∞–Ω, –≥—Ä–∏–±')
    st.write('- –û–±—ä–µ–º –≤—ã–±–æ—Ä–∫–∏: 186 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π')

with st.expander("–ú–µ—Ç—Ä–∏–∫–∏"):
    st.write('## –ú–µ—Ç—Ä–∏–∫–∏:')
    st.write('  - mAP50: ???  - mAP50-95: ???')


label_dict = {
            'cucumber' : 0,
            'eggplant' : 1,
            'mushroom' : 2
            }

ix2cls = {v: k for k, v in label_dict.items()}



def predict(image):
    img = preprocess(image).unsqueeze(0).to(device)
    with torch.no_grad():
        start_time = time.time()
        preds_cls, preds_reg = model(img)
        end_time = time.time()

    pred_class = preds_cls.argmax(dim=1).item()
    img = img.squeeze().permute(1, 2, 0).cpu().numpy()
    img = (img * 255).astype(np.uint8)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø–∏–∫—Å–µ–ª–µ–π –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 255]
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ BGR –¥–ª—è OpenCV

    pred_box_coords = (preds_reg[0] * 227).cpu().detach().numpy().astype('int')
    pred_box = cv2.rectangle(
        img.copy(), 
        (pred_box_coords[0], pred_box_coords[1]),  # top left
        (pred_box_coords[2], pred_box_coords[3]),  # bottom right
        color=(255, 0, 0), thickness=2
    )

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ –≤ RGB –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ Streamlit
    pred_box = cv2.cvtColor(pred_box, cv2.COLOR_BGR2RGB)
    pred_box = pred_box / 255.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –ø–∏–∫—Å–µ–ª–µ–π –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]

    return pred_box, ix2cls[pred_class], end_time - start_time




st.write('## –í–∞—à–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ:')

with st.sidebar:
    uploaded_files = st.file_uploader('–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–≤–æ—é –∫–∞—Ä—Ç–∏–Ω–∫—É', type=['jpg', 'jpeg', 'png'], accept_multiple_files=True)

def display_results(image, pred_label, inference_time):
    st.title(pred_label)
    st.image(image, use_column_width=True)
    st.write(f"–í—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {inference_time:.4f} —Å–µ–∫—É–Ω–¥")


if uploaded_files is not None:
    for uploaded_file in uploaded_files:
        image = Image.open(uploaded_file)
        predicted_img, predicted_class, inference_time = predict(image)
        display_results(predicted_img, predicted_class, inference_time)
